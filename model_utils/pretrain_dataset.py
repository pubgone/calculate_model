# model_ultils/pretrain_dataset.py

import torch
from torch.utils.data import Dataset

class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)  # â†’ List[(prompt, answer)]

    def load_data(self, path):
        """ä» txt åŠ è½½å¹¶è§£æä¸º (prompt, answer) åˆ—è¡¨"""
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue  # è·³è¿‡ç©ºè¡Œ
                if '=' not in line:
                    print(f"âš ï¸ Warning: Line {idx} skipped (no '=' found): '{line}'")
                    continue
                try:
                    # åˆ†å‰²ä¸€æ¬¡ '='ï¼Œé˜²æ­¢å«å¤šä¸ª '=' çš„å¼‚å¸¸æ•°æ®
                    left, right = line.split('=', 1)
                    left = left.strip()
                    right = right.strip()
                    if left and right:  # ç¡®ä¿éç©º
                        samples.append((left + "=", right))  # prompt ä»¥ '=' ç»“å°¾
                except Exception as e:
                    print(f"âš ï¸ Warning: Line {idx} failed to parse: '{line}' | Error: {e}")
                    continue
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        prompt, answer = self.samples[index]  # e.g., ("12+34=", "46")
        bos_token = getattr(self.tokenizer, 'bos_token', '<s>')
        prompt_with_bos = bos_token +prompt
        # ğŸ”¹ Step 1: ç¼–ç  promptï¼ˆå¸¦ BOSï¼‰
        prompt_enc = self.tokenizer(
            prompt_with_bos,
            add_special_tokens=False,      # â† adds <s>
            return_tensors='pt',
            truncation=False,
        )
        prompt_ids = prompt_enc.input_ids.squeeze(0)  # [L_p]

        # ğŸ”¹ Step 2: ç¼–ç  answer + EOSï¼ˆä¸åŠ  BOSï¼é¿å…é‡å¤ï¼‰
        # è·å– EOS token IDï¼ˆé€‚é…ä½ çš„ tokenizerï¼šå¯èƒ½æ˜¯ 2 æˆ– 3ï¼‰
        eos_token = getattr(self.tokenizer, 'eos_token', '</s>')
        # æ‰‹åŠ¨æ‹¼æ¥ answer + EOS
        answer_with_eos = answer + eos_token
        answer_enc = self.tokenizer(
            answer_with_eos,
            add_special_tokens=False,     # â† critical! no extra <s>
            return_tensors='pt',
            truncation=False,
        )
        answer_ids = answer_enc.input_ids.squeeze(0)  # [L_a]

        # ğŸ”¹ Step 3: æ‹¼æ¥ + padding/truncation
        full_ids = torch.cat([prompt_ids, answer_ids], dim=0)
        if len(full_ids) > self.max_length:
            full_ids = full_ids[:self.max_length]
        else:
            pad_len = self.max_length - len(full_ids)
            full_ids = torch.cat([
                full_ids,
                torch.full((pad_len,), self.tokenizer.pad_token_id, dtype=torch.long)
            ])

        # ğŸ”¹ Step 4: æ„é€  labelsï¼ˆä»… answer+EOS åŒºåŸŸæœ‰æ•ˆï¼‰
        labels = torch.full_like(full_ids, -100)  # -100 = ignore in CrossEntropy
        start_ans = len(prompt_ids)
        end_ans = min(start_ans + len(answer_ids), self.max_length)
        labels[start_ans:end_ans] = full_ids[start_ans:end_ans]

        # ğŸ”¹ Step 5: æ„é€ è‡ªå›å½’è¾“å…¥/ç›®æ ‡
        X = full_ids[:-1].clone().detach().long()   # input for model
        Y = labels[1:].clone().detach().long()      # target (with -100)
        loss_mask = (Y != -100).long()              # where to compute loss

        return X, Y, loss_mask