#MSE Loss model_ultils/pretrain_dataset.py
# model_utils/pretrain_dataset.py

import torch
from torch.utils.data import Dataset

class MSEPretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)  # â†’ List[(expr_str: str, target: float)]


    def load_data(self, path):
        """ä» txt åŠ è½½å¹¶è§£æä¸º (expr_str, float_target) åˆ—è¡¨
        è¾“å…¥æ ¼å¼ï¼šexpr = answerï¼ˆå¦‚ "3 + 4 * 2 = 11"ï¼‰
        å†…éƒ¨è½¬æ¢ä¸ºï¼š<compute>expr</compute> â†’ float(answer)
        """
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue  # è·³è¿‡ç©ºè¡Œ
                if '=' not in line:
                    print(f"âš ï¸ Warning: Line {idx} skipped (no '=' found): '{line}'")
                    continue
                try:
                    # åˆ†å‰²ä¸€æ¬¡ '='ï¼Œé˜²æ­¢å«å¤šä¸ª '=' çš„å¼‚å¸¸æ•°æ®
                    left, right = line.split('=', 1)
                    expr = left.strip()
                    ans_str = right.strip()
                    if not expr or not ans_str:
                        print(f"âš ï¸ Warning: Line {idx} has empty expr or answer: '{line}'")
                        continue

                    # ğŸ”¹ å…³é”®ï¼šä»…ç”¨ float() è§£æç­”æ¡ˆï¼Œç¦ç”¨ evalï¼ˆå®‰å…¨ç¬¬ä¸€ï¼ï¼‰
                    try:
                        target = float(ans_str)
                    except ValueError:
                        print(f"âš ï¸ Warning: Line {idx} answer not numeric: '{ans_str}' in '{line}'")
                        continue

                    # âœ… æ„é€ ä»»åŠ¡åŒ–è¡¨è¾¾å¼ï¼š<compute>expr</compute>
                    # æ³¨æ„ï¼šexpr ä¸­çš„ = å·²è¢«å‰¥ç¦»ï¼Œä¸ä¼šå¹²æ‰°
                    task_expr = f"<compute>{expr}</compute>"
                    samples.append((task_expr, target))

                except Exception as e:
                    print(f"âš ï¸ Warning: Line {idx} failed to parse: '{line}' | Error: {e}")
                    continue
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        expr_with_tags, target = self.samples[index]
        bos_token = getattr(self.tokenizer, 'bos_token', '')
        eos_token = getattr(self.tokenizer, 'eos_token', '')
        full_text = bos_token + expr_with_tags + eos_token
    
        enc = self.tokenizer(
            full_text,
            add_special_tokens=False,
            return_tensors='pt',
            truncation=True,
            max_length=self.max_length,
            padding=False
        )
        input_ids = enc.input_ids.squeeze(0)
    
        if input_ids.size(0) < self.max_length:
            pad_len = self.max_length - input_ids.size(0)
            input_ids = torch.cat([
                input_ids,
                torch.full((pad_len,), self.tokenizer.pad_token_id, dtype=torch.long)
            ])
        else:
            input_ids = input_ids[:self.max_length]
        target_tensor = torch.tensor([target], dtype=torch.float32)  # shape: [1]
        return input_ids, target_tensor





# #CE Loss model_ultils/pretrain_dataset.py

class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)  # â†’ List[(prompt, answer)]

    def load_data(self, path):
        """ä» txt åŠ è½½å¹¶è§£æä¸º (prompt, answer) åˆ—è¡¨"""
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue  # è·³è¿‡ç©ºè¡Œ
                if '=' not in line:
                    print(f"âš ï¸ Warning: Line {idx} skipped (no '=' found): '{line}'")
                    continue
                try:
                    # åˆ†å‰²ä¸€æ¬¡ '='ï¼Œé˜²æ­¢å«å¤šä¸ª '=' çš„å¼‚å¸¸æ•°æ®
                    left, right = line.split('=', 1)
                    left = left.strip()
                    right = right.strip()
                    if left and right:  # ç¡®ä¿éç©º
                        samples.append((left + "=", right))  # prompt ä»¥ '=' ç»“å°¾
                except Exception as e:
                    print(f"âš ï¸ Warning: Line {idx} failed to parse: '{line}' | Error: {e}")
                    continue
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        prompt, answer = self.samples[index]  # e.g., ("12+34=", "46")
        bos_token = getattr(self.tokenizer, 'bos_token', '<s>')
        prompt_with_bos = bos_token +prompt
        # ğŸ”¹ Step 1: ç¼–ç  promptï¼ˆå¸¦ BOSï¼‰
        prompt_enc = self.tokenizer(
            prompt_with_bos,
            add_special_tokens=False,      # â† adds <s>
            return_tensors='pt',
            truncation=False,
        )
        prompt_ids = prompt_enc.input_ids.squeeze(0)  # [L_p]

        # ğŸ”¹ Step 2: ç¼–ç  answer + EOSï¼ˆä¸åŠ  BOSï¼é¿å…é‡å¤ï¼‰
        # è·å– EOS token IDï¼ˆé€‚é…ä½ çš„ tokenizerï¼šå¯èƒ½æ˜¯ 2 æˆ– 3ï¼‰
        eos_token = getattr(self.tokenizer, 'eos_token', '</s>')
        # æ‰‹åŠ¨æ‹¼æ¥ answer + EOS
        answer_with_eos = answer + eos_token
        answer_enc = self.tokenizer(
            answer_with_eos,
            add_special_tokens=False,     # â† critical! no extra <s>
            return_tensors='pt',
            truncation=False,
        )
        answer_ids = answer_enc.input_ids.squeeze(0)  # [L_a]

        # ğŸ”¹ Step 3: æ‹¼æ¥ + padding/truncation
        full_ids = torch.cat([prompt_ids, answer_ids], dim=0)
        if len(full_ids) > self.max_length:
            full_ids = full_ids[:self.max_length]
        else:
            pad_len = self.max_length - len(full_ids)
            full_ids = torch.cat([
                full_ids,
                torch.full((pad_len,), self.tokenizer.pad_token_id, dtype=torch.long)
            ])

        # ğŸ”¹ Step 4: æ„é€  labelsï¼ˆä»… answer+EOS åŒºåŸŸæœ‰æ•ˆï¼‰
        labels = torch.full_like(full_ids, -100)  # -100 = ignore in CrossEntropy
        start_ans = len(prompt_ids)
        end_ans = min(start_ans + len(answer_ids), self.max_length)
        labels[start_ans:end_ans] = full_ids[start_ans:end_ans]

        # ğŸ”¹ Step 5: æ„é€ è‡ªå›å½’è¾“å…¥/ç›®æ ‡
        X = full_ids[:-1].clone().detach().long()   # input for model
        Y = labels[1:].clone().detach().long()      # target (with -100)
        loss_mask = (Y != -100).long()              # where to compute loss

        return X, Y, loss_mask