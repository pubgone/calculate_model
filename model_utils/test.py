#MSELoss pretrain_dataset.test
"""
ğŸ§ª æµ‹è¯• PretrainDatasetï¼ˆMSE å›å½’ç‰ˆï¼‰æ˜¯å¦æ­£ç¡®å®ç°ï¼š
   - è¾“å…¥æ ¼å¼ï¼š<compute>expr</compute> å°è£…
   - è¾“å‡ºï¼šinput_ids [L], target: float
   - ä¸ä½¿ç”¨ maskï¼Œé€‚é… MSE loss

ä¾èµ–ï¼š
  - my_tokenizers.hf_math_tokenizer.HFMathTokenizer
  - model_utils.pretrain_dataset.PretrainDataset
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼ˆé€‚é…ä» tests/ æˆ–æ ¹ç›®å½•è¿è¡Œï¼‰
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from my_tokenizers.hf_math_tokenizer import HFMathTokenizer
from model_ultils.pretrain_dataset import PretrainDataset


def create_test_data():
    """ç”Ÿæˆä¸´æ—¶æµ‹è¯•æ•°æ®æ–‡ä»¶"""
    test_data = [
        "1+1=2",
        "5-2=3",
        "1.5+2.5=4.0",
        "-3*2=-6",
        "100-99=1",
        "",               # ç©ºè¡Œï¼ˆåº”è·³è¿‡ï¼‰
        "no_eq_sign",     # æ— æ•ˆï¼ˆæ—  '='ï¼‰
        "a+b=c",          # æ— æ•ˆï¼ˆç­”æ¡ˆéæ•°å­— â†’ åº”è·³è¿‡ï¼‰
        "2^3=8",          # åˆæ³•ï¼ˆè‹¥ tokenizer æ”¯æŒ '^'ï¼‰
    ]
    test_dir = os.path.join(project_root, "data")
    os.makedirs(test_dir, exist_ok=True)
    test_path = os.path.join(test_dir, "test_mse_dataset.txt")
    with open(test_path, 'w', encoding='utf-8') as f:
        for line in test_data:
            f.write(line + "\n")
    return test_path


def test_pretrain_dataset_mse():
    print("ğŸ§ª æ­£åœ¨æµ‹è¯• PretrainDatasetï¼ˆMSE å›å½’æ¨¡å¼ï¼‰...")

    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    test_path = create_test_data()
    print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®: {test_path}")

    # 2. åˆå§‹åŒ– tokenizer å’Œ dataset
    tokenizer = HFMathTokenizer()
    # ç¡®ä¿ tokenizer æœ‰å¿…è¦å±æ€§ï¼ˆå…¼å®¹ä½ çš„å®ç°ï¼‰
    if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
        tokenizer.pad_token = "<pad>"
        print("âš ï¸ Warning: pad_token not set; using '<pad>'")

    dataset = PretrainDataset(
        data_path=test_path,
        tokenizer=tokenizer,
        max_length=32  # è¶³å¤Ÿå®¹çº³ <s><compute>1+1</compute></s>
    )

    print(f"ğŸ“Š Dataset size: {len(dataset)} (expected: 6 valid lines)")

    # 3. æµ‹è¯•ç¬¬ 0 ä¸ªæ ·æœ¬ ("1+1=2" â†’ expr="<compute>1+1</compute>", target=2.0)
    print("\nğŸ” æµ‹è¯•æ ·æœ¬ 0: '1+1=2'")
    input_ids, target = dataset[0]

    # è§£ç  input_idsï¼ˆæŸ¥çœ‹å®é™…è¾“å…¥æ–‡æœ¬ï¼‰
    decoded_input = tokenizer.decode(input_ids.tolist(), skip_special_tokens=False)
    print(f"   Input (input_ids) : {input_ids.tolist()}")
    print(f"   Decoded input     : '{decoded_input}'")
    print(f"   Target (float)    : {target.item():.1f} (dtype={target.dtype})")
        # âœ… å…³é”®ä¿®å¤ï¼šæå‰è®¡ç®— non_pad_count
    pad_id = tokenizer.pad_token_id
    non_pad_count = (input_ids != pad_id).sum().item()
    print(f"   Non-pad tokens    : {input_ids[:non_pad_count].tolist()}")
    
    # å¯é€‰ï¼šæ‰“å°æ¯ä¸ª tokenï¼ˆææœ‰åŠ©äº debugï¼‰
    print(f"   Tokens (decoded):")
    for i, tid in enumerate(input_ids[:non_pad_count]):
        tok = tokenizer.convert_ids_to_tokens([tid])[0]
        print(f"     [{i:2d}] {tid:5d} â†’ '{tok}'")
    # 4. å…³é”®å±æ€§æ£€æŸ¥
    bos_token = getattr(tokenizer, 'bos_token', '<s>')
    eos_token = getattr(tokenizer, 'eos_token', '</s>')
    bos_task_token = getattr(tokenizer, 'bos_task_token', '<compute>')
    eos_task_token = getattr(tokenizer, 'eos_task_token', '</compute>')

    # éªŒè¯ decoded_input æ˜¯å¦åŒ…å«å®Œæ•´ç»“æ„
    expected_substrings = [bos_token, bos_task_token, "1+1", eos_task_token, eos_token]
    missing = [s for s in expected_substrings if s not in decoded_input]

    # 5. æ–­è¨€æ£€æŸ¥ï¼ˆæ ¸å¿ƒéªŒè¯ï¼‰
    try:
        # (a) æ ·æœ¬æ•°é‡æ­£ç¡®ï¼ˆè·³è¿‡ 3 è¡Œï¼šç©ºè¡Œ + no_eq + a+b=cï¼‰
        assert len(dataset) == 6, f"Expected 6 valid samples, got {len(dataset)}"

        # (b) input_ids æ˜¯ LongTensorï¼Œé•¿åº¦ == max_length
        assert isinstance(input_ids, torch.LongTensor), "input_ids must be LongTensor"
        assert input_ids.size(0) == 32, f"input_ids length must be max_length=32, got {input_ids.size(0)}"

        # (c) target æ˜¯ float32 æ ‡é‡
        assert isinstance(target, torch.FloatTensor), f"target must be FloatTensor, got {type(target)}"
        assert target.ndim == 0, f"target must be scalar (0-dim), got {target.ndim}-dim"

        # (d) target å€¼æ­£ç¡®
        assert abs(target.item() - 2.0) < 1e-5, f"target should be 2.0, got {target.item()}"

        # (e) padding æ­£ç¡®ï¼šæœ«å°¾åº”ä¸º pad_token_id
        pad_id = tokenizer.pad_token_id
        last_token = input_ids[-1].item()
        assert last_token == pad_id, f"Last token should be pad_id={pad_id}, got {last_token}"

        # (f) è‡³å°‘æœ‰ä¸€ä¸ªé-pad tokenï¼ˆé˜²æ­¢å…¨ padï¼‰
        non_pad_count = (input_ids != pad_id).sum().item()
        assert non_pad_count > 5, f"Too few non-pad tokens: {non_pad_count}"

        print("\nâœ… æ‰€æœ‰æ–­è¨€é€šè¿‡ï¼PretrainDatasetï¼ˆMSEç‰ˆï¼‰å·¥ä½œæ­£å¸¸ã€‚")

    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

    # 6. æ‰“å° tokenizer å…³é”® token IDsï¼ˆè¯Šæ–­ç”¨ï¼‰
    print("\nâ„¹ï¸ Tokenizer token IDs:")
    for token in ['<s>', '</s>', '<pad>', '<compute>', '</compute>', '1', '+']:
        try:
            tid = tokenizer.convert_tokens_to_ids([token])[0]
            print(f"   '{token}': {tid}")
        except Exception:
            print(f"   '{token}': N/A")

    # 7. é¢å¤–ï¼šæµ‹è¯•è¾¹ç•Œæ ·æœ¬ï¼ˆè´Ÿæ•° & å°æ•°ï¼‰
    print("\nğŸ” æµ‹è¯•æ ·æœ¬ 2: '1.5+2.5=4.0'")
    _, target2 = dataset[2]
    print(f"   Target: {target2.item():.1f}")
    assert abs(target2.item() - 4.0) < 1e-5, "Sample 2 target mismatch"

    print("ğŸ” æµ‹è¯•æ ·æœ¬ 3: '-3*2=-6'")
    _, target3 = dataset[3]
    print(f"   Target: {target3.item():.1f}")
    assert abs(target3.item() + 6.0) < 1e-5, "Sample 3 target mismatch"

    # æ¸…ç†
    os.remove(test_path)
    print(f"\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶ '{test_path}' å·²æ¸…ç†ã€‚")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_pretrain_dataset_mse()
#CE Loss pretrain_dataset.test
# # -*- coding: utf-8 -*-
# """
# æµ‹è¯• PretrainDataset æ˜¯å¦æ­£ç¡®å®ç° left-to-right completion æ¨¡å¼
# ä¾èµ–ï¼š
#   - my_tokenizers.hf_math_tokenizer.HFMathTokenizer
#   - dataset.pretrain_dataset.PretrainDataset
# """

# import os
# import sys
# import torch

# # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼ˆé€‚é…æ ¹ç›®å½•è¿è¡Œï¼‰
# project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# sys.path.insert(0, project_root)

# # âœ… å¯¼å…¥ä½ å·²å®ç°çš„æ¨¡å—ï¼ˆä¸é‡å¤å®šä¹‰ç±»ï¼ï¼‰
# from my_tokenizers.hf_math_tokenizer import HFMathTokenizer
# from model_ultils.pretrain_dataset import PretrainDataset


# def create_test_data():
#     """ç”Ÿæˆä¸´æ—¶æµ‹è¯•æ•°æ®æ–‡ä»¶"""
#     test_data = [
#         "1+1=2",
#         "5-2=3",
#         "1.5+2.5=4.0",
#         "100-99=1",
#         "",               # ç©ºè¡Œï¼ˆåº”è·³è¿‡ï¼‰
#         "invalid_expr",   # æ— æ•ˆè¡Œï¼ˆåº”è·³è¿‡ï¼‰
#         "a+b=c",          # æ— æ•ˆï¼ˆä½ çš„ tokenizer å¯èƒ½ä¸æ”¯æŒï¼Œä½†æ ¼å¼åˆæ³• â†’ ä¼šå°è¯•å¤„ç†ï¼‰
#     ]
#     test_path = os.path.join(project_root, "data", "test_sample.txt")
#     os.makedirs(os.path.dirname(test_path), exist_ok=True)
#     with open(test_path, 'w', encoding='utf-8') as f:
#         for line in test_data:
#             f.write(line + "\n")
#     return test_path


# def test_pretrain_dataset():
#     print("ğŸ§ª æ­£åœ¨æµ‹è¯• PretrainDataset...")

#     # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
#     test_path = create_test_data()
#     print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®: {test_path}")
    
#     # 2. åˆå§‹åŒ– tokenizer å’Œ dataset
#     tokenizer = HFMathTokenizer()
#     dataset = PretrainDataset(
#         data_path=test_path,
#         tokenizer=tokenizer,
#         max_length=11
#     )

#     print(f"ğŸ“Š Dataset size: {len(dataset)} (expected: 5 valid lines)")

#     # 3. æµ‹è¯•ç¬¬ 0 ä¸ªæ ·æœ¬ ("12+34=46" â†’ prompt="12+34=", answer="46")
#     print("\nğŸ” æµ‹è¯•æ ·æœ¬ 0: '1+1=2'")
#     X, Y, loss_mask = dataset[0]

#     # è§£ç  Xï¼ˆæ¨¡å‹è¾“å…¥ï¼‰
#     decoded_input = tokenizer.decode(X.tolist(), skip_special_tokens=False)
#     print(f"   Input (X)  : '{decoded_input}'")

#     # æ‰¾å‡º Y ä¸­å‚ä¸ loss çš„ä½ç½®ï¼ˆY != -100ï¼‰
#     loss_positions = torch.where(Y != -100)[0].tolist()
#     loss_tokens = [Y[i].item() for i in loss_positions]
#     decoded_loss_targets = tokenizer.decode(loss_tokens, skip_special_tokens=False)
#     print(f"   Loss tokens: {loss_tokens} â†’ '{decoded_loss_targets}'")
#     print(f"   Loss mask  : {loss_mask.tolist()}")

#     # âœ… é¢„æœŸï¼šloss åº”è¦†ç›– "46</s>" çš„ token IDsï¼ˆä¾‹å¦‚ [?, ?, 2]ï¼‰
#     eos_id = getattr(tokenizer, 'eos_token_id', tokenizer.convert_tokens_to_ids(['</s>'])[0])
#     print(f"   EOS token ID: {eos_id}")

#     # æ–­è¨€æ£€æŸ¥ï¼ˆå…³é”®ï¼ï¼‰
#     try:
#         assert len(dataset) == 5, f"Expected 5 valid samples, got {len(dataset)}"
#         assert "=" in decoded_input, "Input should contain '='"
#         assert len(loss_positions) > 0, "Should have at least one loss position"
#         assert eos_id in loss_tokens, f"EOS ({eos_id}) should be in loss targets"
#         print("\nâœ… æ‰€æœ‰æ–­è¨€é€šè¿‡ï¼PretrainDataset å·¥ä½œæ­£å¸¸ã€‚")
#     except AssertionError as e:
#         print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
#         sys.exit(1)

#     # 4. å¯é€‰ï¼šæ‰“å° tokenizer å…³é”®å±æ€§ï¼ˆå¸®ä½ éªŒè¯ä¸€è‡´æ€§ï¼‰
#     print("\nâ„¹ï¸ Tokenizer info:")
#     print(f"   vocab_size   : {tokenizer.vocab_size}")
#     print(f"   pad_token_id : {getattr(tokenizer, 'pad_token_id', 'N/A')}")
#     print(f"   bos_token_id : {getattr(tokenizer, 'bos_token_id', 'N/A')}")
#     print(f"   eos_token_id : {eos_id}")
#     print(f"   '4' token ID : {tokenizer.convert_tokens_to_ids(['4'])}")

#     # æ¸…ç†
#     os.remove(test_path)
#     print(f"\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")


# if __name__ == "__main__":
#     test_pretrain_dataset()
