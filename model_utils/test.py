#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• PretrainDataset æ˜¯å¦æ­£ç¡®å®ç° left-to-right completion æ¨¡å¼
ä¾èµ–ï¼š
  - my_tokenizers.hf_math_tokenizer.HFMathTokenizer
  - dataset.pretrain_dataset.PretrainDataset
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼ˆé€‚é…æ ¹ç›®å½•è¿è¡Œï¼‰
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# âœ… å¯¼å…¥ä½ å·²å®ç°çš„æ¨¡å—ï¼ˆä¸é‡å¤å®šä¹‰ç±»ï¼ï¼‰
from my_tokenizers.hf_math_tokenizer import HFMathTokenizer
from model_ultils.pretrain_dataset import PretrainDataset


def create_test_data():
    """ç”Ÿæˆä¸´æ—¶æµ‹è¯•æ•°æ®æ–‡ä»¶"""
    test_data = [
        "1+1=2",
        "5-2=3",
        "1.5+2.5=4.0",
        "100-99=1",
        "",               # ç©ºè¡Œï¼ˆåº”è·³è¿‡ï¼‰
        "invalid_expr",   # æ— æ•ˆè¡Œï¼ˆåº”è·³è¿‡ï¼‰
        "a+b=c",          # æ— æ•ˆï¼ˆä½ çš„ tokenizer å¯èƒ½ä¸æ”¯æŒï¼Œä½†æ ¼å¼åˆæ³• â†’ ä¼šå°è¯•å¤„ç†ï¼‰
    ]
    test_path = os.path.join(project_root, "data", "test_sample.txt")
    os.makedirs(os.path.dirname(test_path), exist_ok=True)
    with open(test_path, 'w', encoding='utf-8') as f:
        for line in test_data:
            f.write(line + "\n")
    return test_path


def test_pretrain_dataset():
    print("ğŸ§ª æ­£åœ¨æµ‹è¯• PretrainDataset...")

    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    test_path = create_test_data()
    print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®: {test_path}")

    # 2. åˆå§‹åŒ– tokenizer å’Œ dataset
    tokenizer = HFMathTokenizer()
    dataset = PretrainDataset(
        data_path=test_path,
        tokenizer=tokenizer,
        max_length=11
    )

    print(f"ğŸ“Š Dataset size: {len(dataset)} (expected: 5 valid lines)")

    # 3. æµ‹è¯•ç¬¬ 0 ä¸ªæ ·æœ¬ ("12+34=46" â†’ prompt="12+34=", answer="46")
    print("\nğŸ” æµ‹è¯•æ ·æœ¬ 0: '1+1=2'")
    X, Y, loss_mask = dataset[0]

    # è§£ç  Xï¼ˆæ¨¡å‹è¾“å…¥ï¼‰
    decoded_input = tokenizer.decode(X.tolist(), skip_special_tokens=False)
    print(f"   Input (X)  : '{decoded_input}'")

    # æ‰¾å‡º Y ä¸­å‚ä¸ loss çš„ä½ç½®ï¼ˆY != -100ï¼‰
    loss_positions = torch.where(Y != -100)[0].tolist()
    loss_tokens = [Y[i].item() for i in loss_positions]
    decoded_loss_targets = tokenizer.decode(loss_tokens, skip_special_tokens=False)
    print(f"   Loss tokens: {loss_tokens} â†’ '{decoded_loss_targets}'")
    print(f"   Loss mask  : {loss_mask.tolist()}")

    # âœ… é¢„æœŸï¼šloss åº”è¦†ç›– "46</s>" çš„ token IDsï¼ˆä¾‹å¦‚ [?, ?, 2]ï¼‰
    eos_id = getattr(tokenizer, 'eos_token_id', tokenizer.convert_tokens_to_ids(['</s>'])[0])
    print(f"   EOS token ID: {eos_id}")

    # æ–­è¨€æ£€æŸ¥ï¼ˆå…³é”®ï¼ï¼‰
    try:
        assert len(dataset) == 5, f"Expected 5 valid samples, got {len(dataset)}"
        assert "=" in decoded_input, "Input should contain '='"
        assert len(loss_positions) > 0, "Should have at least one loss position"
        assert eos_id in loss_tokens, f"EOS ({eos_id}) should be in loss targets"
        print("\nâœ… æ‰€æœ‰æ–­è¨€é€šè¿‡ï¼PretrainDataset å·¥ä½œæ­£å¸¸ã€‚")
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

    # 4. å¯é€‰ï¼šæ‰“å° tokenizer å…³é”®å±æ€§ï¼ˆå¸®ä½ éªŒè¯ä¸€è‡´æ€§ï¼‰
    print("\nâ„¹ï¸ Tokenizer info:")
    print(f"   vocab_size   : {tokenizer.vocab_size}")
    print(f"   pad_token_id : {getattr(tokenizer, 'pad_token_id', 'N/A')}")
    print(f"   bos_token_id : {getattr(tokenizer, 'bos_token_id', 'N/A')}")
    print(f"   eos_token_id : {eos_id}")
    print(f"   '4' token ID : {tokenizer.convert_tokens_to_ids(['4'])}")

    # æ¸…ç†
    os.remove(test_path)
    print(f"\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")


if __name__ == "__main__":
    test_pretrain_dataset()
