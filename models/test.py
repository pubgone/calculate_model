# # åŠ è½½æœ€ç»ˆæ¨¡å‹ï¼ˆæ¨èç”¨äºæ¨ç†ï¼‰
# from models.model_minimind import MiniMindForCausalLM
# from my_tokenizers.hf_math_tokenizer import HFMathTokenizer

# model = MiniMindForCausalLM.from_pretrained("training/2025.10.30/minimind-math-h512-l8")
# tokenizer = HFMathTokenizer.from_pretrained("training/2025.10.30/minimind-math-h512-l8")
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))

from models.model_minimind import MiniMindForCausalLM
from my_tokenizers.hf_math_tokenizer import HFMathTokenizer

# === é…ç½®è·¯å¾„ ===
# è¯·æ ¹æ®ä½ çš„å®é™…ä¿å­˜è·¯å¾„ä¿®æ”¹è¿™é‡Œ
SAVE_DIR = "training/2025.10.30"  # ä½ çš„ args.out_dir
MODEL_NAME = "minimind-math-h512-l8"  # æ ¹æ®ä½ çš„é…ç½®è°ƒæ•´ï¼Œæ¯”å¦‚ h512-l8 æˆ– h512-l8-moe

MODEL_PATH = os.path.join(SAVE_DIR, MODEL_NAME)
TOKENIZER_PATH = os.path.join(SAVE_DIR, "tokenizer")  # æˆ–ç›´æ¥ç”¨ MODEL_PATHï¼ˆå¦‚æœä½ ç”¨äº†æœ€ç»ˆå®Œæ•´æ¨¡å‹ï¼‰

def test_load():
    print("ğŸ” æ­£åœ¨å°è¯•åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

    # 1. åŠ è½½åˆ†è¯å™¨
    try:
        tokenizer = HFMathTokenizer.from_pretrained(TOKENIZER_PATH)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼vocab_size = {tokenizer.vocab_size}")
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return

    # 2. åŠ è½½æ¨¡å‹
    try:
        model = MiniMindForCausalLM.from_pretrained(MODEL_PATH)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"   æ¨¡å‹ vocab_size: {model.config.vocab_size}")
        print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. éªŒè¯ä¸€è‡´æ€§
    if tokenizer.vocab_size != model.config.vocab_size:
        print("âš ï¸ è­¦å‘Šï¼štokenizer ä¸ model çš„ vocab_size ä¸ä¸€è‡´ï¼")
        print(f"   Tokenizer: {tokenizer.vocab_size}, Model: {model.config.vocab_size}")
    else:
        print("âœ… Tokenizer ä¸ Model vocab_size ä¸€è‡´")

    # 4. ç®€å•æ¨ç†æµ‹è¯•
    test_text = "12+34=46"
    try:
        inputs = tokenizer(test_text, return_tensors="pt", add_special_tokens=True)
        print(f"âœ… ç¼–ç æµ‹è¯•: '{test_text}' â†’ {inputs.input_ids.tolist()}")
        
        model.eval()
        with torch.no_grad():
            outputs = model(**inputs)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼logits shape: {outputs.logits.shape}")
        
        # è§£ç é¢„æµ‹
        pred_id = outputs.logits.argmax(dim=-1)[0, -1].item()
        pred_token = tokenizer.decode([pred_id])
        print(f"âœ… é¢„æµ‹ä¸‹ä¸€ä¸ª token: '{pred_token}'")
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å’Œåˆ†è¯å™¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")

if __name__ == "__main__":
    import torch
    test_load()