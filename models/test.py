#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ§ª æµ‹è¯• MiniMindForRegression å›å½’å¤´ï¼ˆé€‚é…å·²é…ç½®å¥½çš„ HFMathTokenizerï¼‰
"""

import os
import sys
import torch

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    from my_tokenizers.hf_math_tokenizer import HFMathTokenizer
    from models.model_minimind import MiniMindConfig, MiniMindForRegression
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def validate_tokenizer(tokenizer):
    """éªŒè¯ tokenizer å·²æ­£ç¡®é…ç½® special tokensï¼ˆåªè¯»æ£€æŸ¥ï¼Œä¸ä¿®æ”¹ï¼‰"""
    required_attrs = ['pad_token_id', 'bos_token_id', 'eos_token_id']
    for attr in required_attrs:
        assert hasattr(tokenizer, attr), f"tokenizer missing {attr}"
        val = getattr(tokenizer, attr)
        assert isinstance(val, int) and val >= 0, f"{attr}={val} invalid"

    # æ£€æŸ¥ <compute> æ˜¯å¦åœ¨ vocab ä¸­
    compute_id = tokenizer.convert_tokens_to_ids(['<compute>'])
    close_compute_id = tokenizer.convert_tokens_to_ids(['</compute>'])
    assert compute_id[0] != tokenizer.unk_token_id, "<compute> not in vocab"
    assert close_compute_id[0] != tokenizer.unk_token_id, "</compute> not in vocab"

    print("âœ… Tokenizer éªŒè¯é€šè¿‡ï¼šspecial tokens å·²æ­£ç¡®é…ç½®")


def test_regression_head():
    print("ğŸ§ª æ­£åœ¨æµ‹è¯•å›å½’å¤´...")

    # 1. åŠ è½½ tokenizerï¼ˆä¸ä¿®æ”¹ï¼ï¼‰
    tokenizer = HFMathTokenizer()
    validate_tokenizer(tokenizer)

    # 2. æ„é€  configï¼ˆIDs ä» tokenizer è¯»å–ï¼Œç¡®ä¿ä¸€è‡´ï¼‰
    config = MiniMindConfig(
        vocab_size=len(tokenizer),
        hidden_size=256,      # å°æ¨¡å‹åŠ é€Ÿæµ‹è¯•
        num_hidden_layers=2,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = MiniMindForRegression(config)
    model.eval()
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ | Layers: {config.num_hidden_layers}, Hidden: {config.hidden_size}")

    # 4. æ„é€ æµ‹è¯•æ ·æœ¬ï¼ˆä½¿ç”¨ tokenizer åŸç”Ÿ encodeï¼‰
    test_expr = "<s><compute>2+3</compute></s>"
    input_ids = tokenizer(test_expr, return_tensors="pt", add_special_tokens=False).input_ids
    print(f"   Input: '{tokenizer.decode(input_ids[0], skip_special_tokens=False)}'")
    print(f"   Shape: {input_ids.shape} | IDs: {input_ids.tolist()}")

    # 5. å‰å‘æµ‹è¯•ï¼ˆå« profilingï¼‰
    with torch.no_grad():
        outputs = model(input_ids, profiling=True)

    # 6. å…³é”®æ–­è¨€
    assert "prediction" in outputs
    pred = outputs["prediction"]
    assert pred.shape == (1,), f"Prediction shape mismatch: {pred.shape}"
    assert not torch.isnan(pred).any(), "Prediction is NaN!"
    assert not torch.isinf(pred).any(), "Prediction is Inf!"

    # 7. Profiling æ£€æŸ¥
    assert "profiling_logs" in outputs, "Missing profiling_logs"
    logs = outputs["profiling_logs"]
    expected_events = config.num_hidden_layers * 4  # attn_in, out, mlp_in, out
    assert len(logs) >= expected_events, f"Too few logs: {len(logs)} < {expected_events}"

    print(f"âœ… é¢„æµ‹å€¼: {pred.item():.3f}")
    print(f"âœ… Profiling logs: {len(logs)} events collected")
    print("ğŸ‰ å›å½’å¤´æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_regression_head()
# # # åŠ è½½æœ€ç»ˆæ¨¡å‹ï¼ˆæ¨èç”¨äºæ¨ç†ï¼‰
# # from models.model_minimind import MiniMindForCausalLM
# # from my_tokenizers.hf_math_tokenizer import HFMathTokenizer

# # model = MiniMindForCausalLM.from_pretrained("training/2025.10.30/minimind-math-h512-l8")
# # tokenizer = HFMathTokenizer.from_pretrained("training/2025.10.30/minimind-math-h512-l8")
# import os
# import sys
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))

# from models.model_minimind import MiniMindForCausalLM
# from my_tokenizers.hf_math_tokenizer import HFMathTokenizer

# # === é…ç½®è·¯å¾„ ===
# # è¯·æ ¹æ®ä½ çš„å®é™…ä¿å­˜è·¯å¾„ä¿®æ”¹è¿™é‡Œ
# SAVE_DIR = "training/2025.10.30"  # ä½ çš„ args.out_dir
# MODEL_NAME = "minimind-math-h512-l8"  # æ ¹æ®ä½ çš„é…ç½®è°ƒæ•´ï¼Œæ¯”å¦‚ h512-l8 æˆ– h512-l8-moe

# MODEL_PATH = os.path.join(SAVE_DIR, MODEL_NAME)
# TOKENIZER_PATH = os.path.join(SAVE_DIR, "tokenizer")  # æˆ–ç›´æ¥ç”¨ MODEL_PATHï¼ˆå¦‚æœä½ ç”¨äº†æœ€ç»ˆå®Œæ•´æ¨¡å‹ï¼‰

# def test_load():
#     print("ğŸ” æ­£åœ¨å°è¯•åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

#     # 1. åŠ è½½åˆ†è¯å™¨
#     try:
#         tokenizer = HFMathTokenizer.from_pretrained(TOKENIZER_PATH)
#         print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼vocab_size = {tokenizer.vocab_size}")
#     except Exception as e:
#         print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
#         return

#     # 2. åŠ è½½æ¨¡å‹
#     try:
#         model = MiniMindForCausalLM.from_pretrained(MODEL_PATH)
#         print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼æ¨¡å‹ç±»å‹: {type(model).__name__}")
#         print(f"   æ¨¡å‹ vocab_size: {model.config.vocab_size}")
#         print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
#     except Exception as e:
#         print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
#         return

#     # 3. éªŒè¯ä¸€è‡´æ€§
#     if tokenizer.vocab_size != model.config.vocab_size:
#         print("âš ï¸ è­¦å‘Šï¼štokenizer ä¸ model çš„ vocab_size ä¸ä¸€è‡´ï¼")
#         print(f"   Tokenizer: {tokenizer.vocab_size}, Model: {model.config.vocab_size}")
#     else:
#         print("âœ… Tokenizer ä¸ Model vocab_size ä¸€è‡´")

#     # 4. ç®€å•æ¨ç†æµ‹è¯•
#     test_text = "12+34=46"
#     try:
#         inputs = tokenizer(test_text, return_tensors="pt", add_special_tokens=True)
#         print(f"âœ… ç¼–ç æµ‹è¯•: '{test_text}' â†’ {inputs.input_ids.tolist()}")
        
#         model.eval()
#         with torch.no_grad():
#             outputs = model(**inputs)
#         print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼logits shape: {outputs.logits.shape}")
        
#         # è§£ç é¢„æµ‹
#         pred_id = outputs.logits.argmax(dim=-1)[0, -1].item()
#         pred_token = tokenizer.decode([pred_id])
#         print(f"âœ… é¢„æµ‹ä¸‹ä¸€ä¸ª token: '{pred_token}'")
        
#     except Exception as e:
#         print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
#         return

#     print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å’Œåˆ†è¯å™¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")

# if __name__ == "__main__":
#     import torch
#     test_load()