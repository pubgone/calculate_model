import os
import torch

# âœ… æ­£ç¡®å¯¼å…¥æ–¹å¼ï¼ˆåŒçº§ç›®å½•ï¼‰
from hf_math_tokenizer import HFMathTokenizer

print("âœ… æ­£åœ¨åˆ›å»º HFMathTokenizer...")

tokenizer = HFMathTokenizer()
print(f"âœ… è¯è¡¨å¤§å°: {tokenizer.vocab_size}")

text = "sin(pi*x)+log(e)=1"
print(f"\nğŸ“ æµ‹è¯•æ–‡æœ¬: {text}")
encoded = tokenizer(text, return_tensors="pt")
print(f"ğŸ”¢ Input IDs shape: {encoded.input_ids.shape}")
print(f"ğŸ”¢ Input IDs: {encoded.input_ids.tolist()}")

decoded = tokenizer.decode(encoded.input_ids[0], skip_special_tokens=False)
print(f"ğŸ”¤ è§£ç ç»“æœ: {repr(decoded)}")

# ä¿å­˜åˆ°å½“å‰ç›®å½•
save_dir = "./saved_hf_tokenizer"
os.makedirs(save_dir, exist_ok=True)
print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {save_dir}")
tokenizer.save_pretrained(save_dir)

print("\nğŸ‰ æˆåŠŸï¼è¯·æ£€æŸ¥ ./saved_hf_tokenizer/ ç›®å½•ä¸‹çš„æ–‡ä»¶ã€‚")